from pyspark.sql import SparkSession
from datetime import datetime
import json
import pytz
import boto3
import pandas as pd
 
json_file_path = 's3://s3/SampleDB_Conn.json'
csv_output_path = 's3://s3/DB_connection_info.csv'
 
# Initialize Spark session
spark = SparkSession.builder.appName("JDBCConnectionTest").getOrCreate()
 
# Initialize Glue client
glue_client = boto3.client('glue')
connections_data = pd.read_json(json_file_path)
 
# List to store connection information
connection_info = []
 
# Loop through each database.table object and test JDBC connections using Spark
for table_key, table_info in connections_data.items():
    if table_key != "env":
        try:
            connection_name = table_info['conn_name']
            database_name = table_info['databaseName']
            table_name = table_info['dbtable']
            try:
                glue_connection = glue_client.get_connection(Name=connection_name)['Connection']
                # Extract the JDBC URL from the Glue connection
                jdbc_url = glue_connection['ConnectionProperties']['JDBC_CONNECTION_URL']
                username = glue_connection['ConnectionProperties']['USERNAME']
                password = glue_connection['ConnectionProperties']['PASSWORD']
 
                # Attempt to establish the connection
                try:
                    jdbc_df = spark.read \
                        .format("jdbc") \
                        .option("url", jdbc_url) \
                        .option("dbtable", table_name) \
                        .option("user", username) \
                        .option("password", password) \
                        .load()
                    # Update connection status to "success"
                    connection_status = "success"
                    has_table = "Yes"
                    has_data = "Yes" if not jdbc_df.isEmpty() else "No"
                    connection_info.append((connection_name, connection_status, table_key, has_table, has_data))
                    print(connection_name, connection_status, table_key, has_table, has_data)
                except Exception as e:
                    # Append connection information to the list indicating an error
                    connection_status = "failure_in_jdbc"
                    if "doesn't exist" in str(e):
                        has_table = 'No'
                    else:
                        has_table = 'N/A'
                   
                    connection_info.append((connection_name, connection_status, table_key, has_table, "N/A"))
                    print(connection_name, connection_status, table_key, has_table, "N/A")
 
            except Exception as e:
                # Append connection information to the list indicating a connection issue
                connection_info.append((connection_name, "failure_glue_connection", table_key, "N/A", "N/A"))
                print(connection_name, "failure_glue_connection", table_key, "N/A", "N/A")
        except KeyError as e:
            # Append connection information to the list indicating a KeyError
            connection_info.append((connection_name, "failure_reading_json", table_key, "N/A", "N/A"))
            print(connection_name, "failure_reading_json", table_key, "N/A", "N/A")
 
# Create a DataFrame from the connection information
columns = ["Connection", "Connection Status", "Table Key", "Has Table", "Has Data"]
connection_df = pd.DataFrame(connection_info, columns=columns)
 
# Write the DataFrame to a CSV file
connection_df.to_csv(csv_output_path, index=False)
 
tz_NY = pytz.timezone('America/New_York')
datetime_NY = datetime.now(tz_NY)
 
print("Connection information written to CSV : ", csv_output_path )
print('Output file generated on : ', datetime_NY.strftime("%Y-%m-%d %H:%M:%S"))


###########################################################################################################


from pyspark.sql import SparkSession
from datetime import datetime
import json
import pytz
import boto3
import pandas as pd
from awsglue.utils import getResolvedOptions
import sys
import logging

def establish_spark_session(app_name):
    return SparkSession.builder.appName(app_name).getOrCreate()

def get_connection_info(glue_client, table_info):
    try:
        connection_name = table_info['conn_name']
        database_name = table_info['databaseName']
        table_name = table_info['dbtable']

        # Check if the connection exists in AWS Glue
        glue_connection = glue_client.get_connection(Name=connection_name)['Connection']

        jdbc_url = glue_connection['ConnectionProperties']['JDBC_CONNECTION_URL']
        username = glue_connection['ConnectionProperties']['USERNAME']
        password = glue_connection['ConnectionProperties']['PASSWORD']

        try:
            jdbc_df = spark.read \
                .format("jdbc") \
                .option("url", jdbc_url) \
                .option("dbtable", table_name) \
                .option("user", username) \
                .option("password", password) \
                .load()

            connection_status = "success"
            has_table = "Yes"
            has_data = "Yes" if not jdbc_df.isEmpty() else "No"
        except Exception as e:
            connection_status = "failure_in_jdbc"
            has_table = 'No' if "doesn't exist" in str(e) else 'N/A'
    except Exception as e:
        connection_status = "failure_glue_connection"
        has_table = "N/A"
        # Handle the case where the Glue connection is not found
        if "not found" in str(e):
            connection_status = "Connection_name not found in Glue connection"

    return connection_name, connection_status, table_key, has_table, "N/A"

def main():
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 's3_bucket', 'input_json_file', 'output_csv_file'])
    s3_bucket = args['s3_bucket']
    input_json = args['input_json_file']
    csv_output = args['output_csv_file']
    input_json_file = f's3://{s3_bucket}/{input_json}'
    csv_output_path = f's3://{s3_bucket}/{csv_output}'

    spark = establish_spark_session("JDBCConnectionTest")
    glue_client = boto3.client('glue')

    connections_data = pd.read_json(input_json_file)
    connection_info = []

    for table_key, table_info in connections_data.items():
        if table_key != "env":
            connection_info.append(get_connection_info(glue_client, table_info))

    columns = ["Connection", "Connection Status", "Table Key", "Has Table", "Has Data"]
    connection_df = pd.DataFrame(connection_info, columns=columns)
    connection_df.to_csv(csv_output_path, index=False)

    tz_NY = pytz.timezone('America/New_York')
    datetime_NY = datetime.now(tz_NY)

    logging.basicConfig(filename='connection_log.txt', level=logging.INFO)
    logging.info("Connection information written to CSV: %s", csv_output_path)
    logging.info("Output file generated on: %s", datetime_NY.strftime("%Y-%m-%d %H:%M:%S"))

if __name__ == "__main__":
    main()
