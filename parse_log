import re
import pandas as pd
import boto3
from urllib.parse import urlparse

# AWS S3 URIs
WWD_URI = 'S3_location'
WWD_path_prefix_to_remove = 'some_string'

Glue_URI = 'S3_location_2'
Glue_path_prefix_to_remove = 'some_string'

s3_client = boto3.client('s3')

table_pattern = re.compile(r"Library (\w+) assigned with PARQUET engine\. Found (\d+) table\(s\) in (\d+\.\d+) seconds\.")
dataframe_pattern = re.compile(r"DataFrame (\S+) \((\d+) rows, (\d+) columns\) saved in [\d.]+ seconds to:\s+'(.+)'")

def read_log_from_s3(s3_uri):
    parsed_uri = urlparse(s3_uri)
    s3_bucket = parsed_uri.netloc
    s3_key = parsed_uri.path.lstrip('/')
    response = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)
    log_content = response['Body'].read().decode('utf-8')
    return log_content

def extract_matches(log_content, pattern):
    return pattern.findall(log_content)

def get_metadata_df(source, log_content, path_prefix_to_remove):
    table_matches = extract_matches(log_content, table_pattern)
    table_data = [(library_name, count) for library_name, count, _ in table_matches]

    dataframe_matches = extract_matches(log_content, dataframe_pattern)
    dataframe_data = [(dataframe_name, rows, columns, file_path.replace(path_prefix_to_remove, '')) for dataframe_name, rows, columns, file_path in dataframe_matches]

    table_df = pd.DataFrame(table_data, columns=['library_name', 'count']).add_suffix(f'_{source}')
    dataframe_df = pd.DataFrame(dataframe_data, columns=['dataframe_name', 'rows', 'columns', 'file_path']).add_suffix(f'_{source}')

    return table_df, dataframe_df

# Read log content and extract metadata for WWD
WWD_log_content = read_log_from_s3(WWD_URI)
WWD_table_df, WWD_dataframe_df = get_metadata_df('WWD', WWD_log_content, WWD_path_prefix_to_remove)

# Read log content and extract metadata for Glue
Glue_log_content = read_log_from_s3(Glue_URI)
Glue_table_df, Glue_dataframe_df = get_metadata_df('Glue', Glue_log_content, Glue_path_prefix_to_remove)

# Merge and write to S3
table_merged_df = pd.merge(WWD_table_df, Glue_table_df, left_on=['library_name_WWD'], right_on=['library_name_Glue'], how='left')
table_merged_df.to_csv('s3://table_comparison.csv', index=False)

dataframe_merged_df = pd.merge(WWD_dataframe_df, Glue_dataframe_df, left_on=['dataframe_name_WWD'], right_on=['dataframe_name_Glue'], how='left')
dataframe_merged_df.to_csv('s3://files_comparison.csv', index=False)

print("Done!")



######################### adding count to avoid dups 
import re
import pandas as pd
import boto3
from urllib.parse import urlparse

# AWS S3 URIs
WWD_URI = 'S3_location'
WWD_path_prefix_to_remove = 'some_string'

Glue_URI = 'S3_location_2'
Glue_path_prefix_to_remove = 'some_string'

s3_client = boto3.client('s3')

table_pattern = re.compile(r"Library (\w+) assigned with PARQUET engine\. Found (\d+) table\(s\) in (\d+\.\d+) seconds\.")
dataframe_pattern = re.compile(r"DataFrame (\S+) \((\d+) rows, (\d+) columns\) saved in [\d.]+ seconds to:\s+'(.+)'")

def read_log_from_s3(s3_uri):
    parsed_uri = urlparse(s3_uri)
    s3_bucket = parsed_uri.netloc
    s3_key = parsed_uri.path.lstrip('/')
    response = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)
    log_content = response['Body'].read().decode('utf-8')
    return log_content

def extract_matches(log_content, pattern):
    return pattern.findall(log_content)

def get_metadata_df(source, log_content, path_prefix_to_remove):
    table_matches = extract_matches(log_content, table_pattern)
    table_data = [(f'{library_name}_{source}_{i}', count) for i, (library_name, count, _) in enumerate(table_matches, start=1)]

    dataframe_matches = extract_matches(log_content, dataframe_pattern)
    dataframe_data = [(f'{dataframe_name}_{source}_{i}', rows, columns, file_path.replace(path_prefix_to_remove, '')) for i, (dataframe_name, rows, columns, file_path) in enumerate(dataframe_matches, start=1)]

    table_df = pd.DataFrame(table_data, columns=[f'library_name_{source}', f'count_{source}'])
    dataframe_df = pd.DataFrame(dataframe_data, columns=[f'dataframe_name_{source}', f'rows_{source}', f'columns_{source}', f'file_path_{source}'])

    return table_df, dataframe_df

# Read log content and extract metadata for WWD
WWD_log_content = read_log_from_s3(WWD_URI)
WWD_table_df, WWD_dataframe_df = get_metadata_df('WWD', WWD_log_content, WWD_path_prefix_to_remove)

# Read log content and extract metadata for Glue
Glue_log_content = read_log_from_s3(Glue_URI)
Glue_table_df, Glue_dataframe_df = get_metadata_df('Glue', Glue_log_content, Glue_path_prefix_to_remove)

# Merge and write to S3
table_merged_df = pd.merge(WWD_table_df, Glue_table_df, left_on=[f'library_name_WWD'], right_on=[f'library_name_Glue'], how='left')
table_merged_df['count_diff'] = table_merged_df.apply(lambda row: 'Yes' if row['count_WWD'] != row['count_Glue'] else 'No', axis=1)
table_merged_df.to_csv('s3://table_comparison.csv', index=False)

dataframe_merged_df = pd.merge(WWD_dataframe_df, Glue_dataframe_df, left_on=[f'dataframe_name_WWD'], right_on=[f'dataframe_name_Glue'], how='left')

# Merge and compare rows_WWD, columns_WWD, file_path_WWD, and file_path_Glue columns
dataframe_merged_df['rows_diff'] = dataframe_merged_df.apply(lambda row: 'Yes' if row['rows_WWD'] != row['rows_Glue'] else 'No', axis=1)
dataframe_merged_df['columns_diff'] = dataframe_merged_df.apply(lambda row: 'Yes' if row['columns_WWD'] != row['columns_Glue'] else 'No', axis=1)
dataframe_merged_df['file_path_diff'] = dataframe_merged_df.apply(lambda row: 'Yes' if row['file_path_WWD'] != row['file_path_Glue'] else 'No', axis=1)

dataframe_merged_df.to_csv('s3://files_comparison.csv', index=False)

print("Done!")




'message\n" \\ ^ /    ___ ___  ____ ____ ____ _  _ ____ ___    ____ _  _ _  _ ___ _ _  _ ____\n–< S >–  [__  |__] |__/ |  | |    |_/  |___  |     |__/ |  | |\\ |  |  | |\\/| |___ \n / v \\   ___] |    |  \\ |__| |___ | \\_ |___  |     |  \\ |__| | \\|  |  | |  | |___ \nv3.2.4                                                            By WiseWithData\n"\n"NOTE: Library work assigned with PARQUET engine. Found 0 table(s) in 0.06 seconds.\n"\n"NOTE: Library sasuser assigned with PARQUET engine. Found 0 table(s) in 0.05 seconds.\nNOTE: SPROCKET Runtime initialized\n"\n"start Time = 29Aug23:07:54:50\n"\n"NOTE: Library cif assigned with PARQUET engine. Found 2 table(s) in 5.82 seconds.\n"\n"NOTE: Library slf assigned with PARQUET engine. Found 2 table(s) in 4.42 seconds.\n"\n"NOTE: Library auto assigned with PARQUET engine. Found 3 table(s) in 5.00 seconds.\n"\n"NOTE: Library tbl assigned with PARQUET engine. Found 13 table(s) in 8.08 seconds.\n"\n"23\n"\n"202307\n"\n"jul\njul-23\n"\n"31jul2023\n"\n"

 

NOTE: DataFrame tbl.sponsor_penetration_jul (0 rows, 5 columns) saved in 1.62 seconds to:

\n      \'s3://s3-path//sponsor_penetration_jul.parquet\'\n"\n"

 

NOTE: DataFrame tbl.sponsor_penetration (11,000 rows, 5 columns) saved in 7.28 seconds to:

\n      \'s3://s3-path//sponsor_penetration.parquet\'

 

\n"\n"End time = 29Aug23:07:55:49\n"'
