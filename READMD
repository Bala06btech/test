import boto3
import pandas as pd
from datetime import datetime, timezone

bucket_name = '<bucket>'
input_file = '<input_path>'
output_file = '<output_path>'

# Set the filtering time in your local timezone
filter_time = datetime.strptime('2023-07-30 01:30:00', '%Y-%m-%d %H:%M:%S')

# Create an S3 client
s3 = boto3.client('s3')

def check_folder_available(folder_key):
    try:
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key, Delimiter='/')
        return 'Yes' if any(obj['LastModified'].replace(tzinfo=None) >= filter_time for obj in response.get('Contents', [])) else 'No'
    except Exception as e:
        return 'No'

def check_file_available(object_key):
    try:
        obj = s3.head_object(Bucket=bucket_name, Key=object_key)
        return 'Yes' if obj['LastModified'].replace(tzinfo=None) >= filter_time else 'No'
    except Exception as e:
        return 'No'

# Read input file from S3 into a list of lines
response = s3.get_object(Bucket=bucket_name, Key=input_file)
lines = response['Body'].read().decode('utf-8').splitlines()

# Check availability for each object
results = []

for line in lines:
    object_key = line.strip()

    if object_key.endswith('/'):
        result = check_folder_available(object_key)
    else:
        result = check_file_available(object_key)

    results.append((object_key, result))

# Create a Pandas DataFrame
pandas_df = pd.DataFrame(results, columns=['file_name', 'file_available'])

# Save DataFrame to CSV
pandas_df.to_csv(output_file, index=False)
print('Check the output file')





//Check S3 files available check //

import boto3
import pandas as pd

bucket_name = '<bucket>'
input_file = '<input_path>'
output_file = '<output_path>'

# Create an S3 client
s3 = boto3.client('s3')

def check_folder_available(folder_key):
    try:
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key, Delimiter='/')
        return 'Yes' if 'Contents' in response or 'CommonPrefixes' in response else 'No'
    except Exception as e:
        return 'No'

def check_file_available(object_key):
    try:
        s3.head_object(Bucket=bucket_name, Key=object_key)
        return 'Yes'
    except Exception as e:
        return 'No'

# Read input file into a list
with open(input_file, 'r') as f:
    lines = f.readlines()

# Check availability for each object
results = []

for line in lines:
    object_key = line.strip()

    if object_key.endswith('/'):
        result = check_folder_available(object_key)
    else:
        result = check_file_available(object_key)

    results.append((object_key, result))

# Create a Pandas DataFrame
pandas_df = pd.DataFrame(results, columns=['file_name', 'file_available'])

# Save DataFrame to CSV
pandas_df.to_csv(output_file, index=False)
print('Check the output file')



//Files in subfolders//
import boto3
# connect to s3 - assuming your creds are all set up and you have boto3 installed
s3 = boto3.client('s3')

# get the bucket
s3_bucket = '<bucket>'
s3_prefix = '<input_path>'

def get_file_count(s3_prefix):
    response =  s3.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)
    file_count = 0
    for obj in response.get("Contents", []):
        if not obj["Key"].endswith("/"):
            file_count +=1
    print(s3_prefix, '|', file_count)

def list_first_level_subfolders(s3_prefix):
    response = s3.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix, Delimiter='/')
    subfolders = [common_prefix["Prefix"] for common_prefix in response.get("CommonPrefixes", [])]
    return subfolders

first_level_subfolders = list_first_level_subfolders(s3_prefix)

for subfolders in first_level_subfolders:
    get_file_count(subfolders)
