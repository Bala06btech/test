import sys
import boto3
import pandas as pd
bucket_name = '<bucket>'
input_file = '<input_path>'
output_file = '<output_path>'

lines = spark.read.text(input_file)

def check_file_available(object_key):
    s3 = boto3.client('s3')
    try:
        s3.head_object(Bucket=bucket_name, Key=object_key)
        return 'Yes'
    except Exception as e:
        return 'No'

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

check_file_udf = udf(check_file_available, StringType())
file_available_df = lines.withColumn('file_available', check_file_udf(lines['value']))

pandas_df = file_available_df.toPandas()
pandas_df.rename(columns={'value' : 'file_name'}, inplace=True)
pandas_df.to_csv(output_file, index=False)
print('Check the output file')
